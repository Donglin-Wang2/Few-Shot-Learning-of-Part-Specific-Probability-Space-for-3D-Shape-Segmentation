{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/sg_optimize.py:8: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import Utils\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import sugartensor as tf # Please use pip install for this small package\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "colorsMap='jet'\n",
    "import matplotlib\n",
    "import matplotlib.cm as cmx\n",
    "import glob\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "train_num=10000\n",
    "batSize=16\n",
    "maxStep=50000 # fixed with learningRate and learningRateDecay\n",
    "learningRate=0.001\n",
    "learningRateDecay=0.995\n",
    "lrType=1 #learning rate type\n",
    "lr2Period=5\n",
    "adam_beta1=0.9 # check adam optimization\n",
    "adam_beta2=0.99\n",
    "conWeightVar=['NNReg'] # variables to be loaded\n",
    "saveStep=200 # frequency to save weight\n",
    "maxKeepWeights=2000 # how many records to save (for disk)\n",
    "stepsContinue=100000  # from which steps continu.\n",
    "# conWeightPath=\"./NIPS-partseg-v1-temp10-chair/Exp4_maxStep_100000/\"\n",
    "conWeightPath=\"\"\n",
    "#For Debug and results printing\n",
    "keepProb=0.99999\n",
    "# Totally dosen't work if put dropout after max-pool layer. \n",
    "printStep=500\n",
    "\n",
    "clas=\"mug-seg\"\n",
    "s1=2048\n",
    "tfname=\"./mug_N2048.tfrecords\"\n",
    "dat=\"LX-run1\"\n",
    "lambda_ratio = 100.0 # init weight for chamfer distance \n",
    "beta_ratio = 0.1 # weight for correlation loss\n",
    "\n",
    "dirSave=\"./{}_trNum_{}_maxStep_{}_lambda_{}\".format(clas,\n",
    "    dat,train_num, maxStep,lambda_ratio)\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:148: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:149: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:165: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "(16, 2048, 3) (16, 2048) (16, 2048, 3) (16, 2048)\n"
     ]
    }
   ],
   "source": [
    "y,l,x,xl=Utils.read_from_tfrecords(tfname,\n",
    "                                 [\"D\",\"P\",\"T\",\"TP\"], batSize, [[s1,3],[s1],[s1,3],[s1]])\n",
    "print(y.shape, l.shape, x.shape, xl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_dist(xt,y_p):\n",
    "    a=xt.shape[1]\n",
    "    b=y_p.shape[1]\n",
    "    dist=tf.tile(tf.expand_dims(y_p,1),[1,a,1,1])-tf.tile(tf.expand_dims(xt,2),[1,1,b,1])\n",
    "    dist=(dist[:,:,:,0]**2+dist[:,:,:,1]**2)\n",
    "    return dist\n",
    "\n",
    "def chamfer_loss(A,B):    \n",
    "    r=tf.reduce_sum(A*A,2)\n",
    "    r=tf.reshape(r,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    r2=tf.reduce_sum(B*B,2)\n",
    "    r2=tf.reshape(r2,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    t=(r-2*tf.matmul(A, tf.transpose(B,perm=[0, 2, 1])) + tf.transpose(r2,perm=[0, 2, 1]))\n",
    "    return tf.reduce_mean((tf.reduce_min(t, axis=1)+tf.reduce_min(t,axis=2))/2.0)\n",
    "\n",
    "def Net(aa, yt, x):\n",
    "    s=aa.shape[1]\n",
    "    with tf.sg_context(name='NNReg', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        yt=tf.expand_dims(yt,2)\n",
    "        \n",
    "        v1=tf.expand_dims(x,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)\n",
    "        print('v1', v1.shape)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)\n",
    "        print('v2', v2.shape)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True)\n",
    "        print('v3', v3.shape) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True)\n",
    "        print('v4', v4.shape) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True)\n",
    "        print('v5', v5.shape) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        print('v5_tiled', v5.shape)\n",
    "        vv5=v5\n",
    "        \n",
    "        v1=yt.sg_conv(dim=16, size=(1,1),  name='gen99',pad=\"SAME\",bn=True)\n",
    "        print('v5', v1.shape)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1),  name='gen11',pad=\"SAME\",bn=True)\n",
    "        print('v5', v2.shape)\n",
    "        v3=v2.sg_conv(dim=128, size=(1,1),  name='gen22',pad=\"SAME\",bn=True)\n",
    "        print('v5', v3.shape)\n",
    "        v4=v3.sg_conv(dim=512, size=(1,1),  name='gen33',pad=\"SAME\",bn=True)\n",
    "        print('v5', v4.shape)\n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1),  name='gen44',pad=\"SAME\",bn=True)\n",
    "        print('v5', v5.shape)\n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        print('v5tile', v5.shape)\n",
    "        ff=tf.concat([tf.expand_dims(aa,2),v5], axis=-1)\n",
    "        print(ff)\n",
    "        ff=tf.concat([ff,vv5], axis=-1)\n",
    "        print('ff2', ff)\n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)\n",
    "        print('f1', f1)  \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        print('f2', f2)\n",
    "        f3=f2.sg_conv(dim=3, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\")\n",
    "        print('f3', f3)  \n",
    "        f3=tf.squeeze(f3,axis=2)\n",
    "        print('f3_out', f3)\n",
    "        \n",
    "    return f3\n",
    "\n",
    "def part_set(y):#used in training, deformed shape\n",
    "    with tf.sg_context(name='RegSeg1', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        v1=tf.expand_dims(y,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        print('v4', v4)\n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        print('v5b', v5)\n",
    "        v5 = tf.reduce_max(v5, axis=1)\n",
    "        print('v5r', v5)\n",
    "        v5 = tf.expand_dims(v5,axis=1)\n",
    "        print('v5e', v5)\n",
    "        v5=tf.tile(v5,[1,int(y.shape[1]),1,1]) #[B,N,1,1024]\n",
    "        print('v5a', v5)\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(y,2),v5], axis=-1) #[B,N,1,3+1024]\n",
    "        print('ff', ff)\n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)\n",
    "        print(f1)  \n",
    "        f1=tf.concat([tf.expand_dims(y,2),f1], axis=-1) \n",
    "        print(f1)\n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True) \n",
    "        print(f2) \n",
    "        f2=tf.concat([tf.expand_dims(y,2),f2], axis=-1) \n",
    "        print(f2)\n",
    "        feat=f2.sg_conv(dim=128, size=(1,1),  name='feat',pad=\"SAME\",bn=True)\n",
    "        print(feat)\n",
    "        f3=feat.sg_conv(dim=4, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\") \n",
    "        print(f3)\n",
    "        f3=tf.squeeze(f3,axis=2) #[B,N,4]\n",
    "        print(f3)\n",
    "        feat = tf.squeeze(feat,axis=2)\n",
    "        print(feat)\n",
    "    return f3, feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/sg_activation.py:40: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "v1 (16, 2048, 1, 16)\n",
      "v2 (16, 2048, 1, 64)\n",
      "v3 (16, 2048, 1, 128)\n",
      "v4 (16, 2048, 1, 512)\n",
      "v5 (16, 2048, 1, 1024)\n",
      "v5_tiled (16, 2048, 1, 1024)\n",
      "v5 (16, 2048, 1, 16)\n",
      "v5 (16, 2048, 1, 64)\n",
      "v5 (16, 2048, 1, 128)\n",
      "v5 (16, 2048, 1, 512)\n",
      "v5 (16, 2048, 1, 1024)\n",
      "v5tile (16, 2048, 1, 1024)\n",
      "Tensor(\"NNReg/concat:0\", shape=(16, 2048, 1, 1027), dtype=float32)\n",
      "ff2 Tensor(\"NNReg/concat_1:0\", shape=(16, 2048, 1, 2051), dtype=float32)\n",
      "f1 Tensor(\"NNReg/f1/out:0\", shape=(16, 2048, 1, 256), dtype=float32)\n",
      "f2 Tensor(\"NNReg/f2/out:0\", shape=(16, 2048, 1, 128), dtype=float32)\n",
      "f3 Tensor(\"NNReg/f3/out:0\", shape=(16, 2048, 1, 3), dtype=float32)\n",
      "f3_out Tensor(\"NNReg/Squeeze:0\", shape=(16, 2048, 3), dtype=float32)\n",
      "(16, 2048, 3)\n",
      "=============================================\n",
      "v4 Tensor(\"RegSeg1/gen3/out:0\", shape=(16, 2048, 1, 512), dtype=float32)\n",
      "v5b Tensor(\"RegSeg1/gen4/out:0\", shape=(16, 2048, 1, 1024), dtype=float32)\n",
      "v5r Tensor(\"RegSeg1/Max:0\", shape=(16, 1, 1024), dtype=float32)\n",
      "v5e Tensor(\"RegSeg1/ExpandDims_1:0\", shape=(16, 1, 1, 1024), dtype=float32)\n",
      "v5a Tensor(\"RegSeg1/Tile:0\", shape=(16, 2048, 1, 1024), dtype=float32)\n",
      "ff Tensor(\"RegSeg1/concat:0\", shape=(16, 2048, 1, 1027), dtype=float32)\n",
      "Tensor(\"RegSeg1/f1/out:0\", shape=(16, 2048, 1, 256), dtype=float32)\n",
      "Tensor(\"RegSeg1/concat_1:0\", shape=(16, 2048, 1, 259), dtype=float32)\n",
      "Tensor(\"RegSeg1/f2/out:0\", shape=(16, 2048, 1, 128), dtype=float32)\n",
      "Tensor(\"RegSeg1/concat_2:0\", shape=(16, 2048, 1, 131), dtype=float32)\n",
      "Tensor(\"RegSeg1/feat/out:0\", shape=(16, 2048, 1, 128), dtype=float32)\n",
      "Tensor(\"RegSeg1/f3/out:0\", shape=(16, 2048, 1, 4), dtype=float32)\n",
      "Tensor(\"RegSeg1/Squeeze:0\", shape=(16, 2048, 4), dtype=float32)\n",
      "Tensor(\"RegSeg1/Squeeze_1:0\", shape=(16, 2048, 128), dtype=float32)\n",
      "(16, 2048, 4) (16, 2048, 128)\n"
     ]
    }
   ],
   "source": [
    "net_out = Net(x,x,y) + x\n",
    "print(net_out.shape)\n",
    "print(\"=============================================\")\n",
    "out1, out2 = part_set(net_out)\n",
    "\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step = tf.Variable(1, trainable=False,name='global_step')\n",
    "tf.train.exponential_decay(lambda_ratio, global_step,\n",
    "                                                  batSize, 0.999, staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/__init__.py:2: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/sg_optimize.py:8: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#Author\n",
    "#Lingjing Wang and Xiang Li\n",
    "#Date: June 2020\n",
    "############################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import Utils\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import sugartensor as tf # Please use pip install for this small package\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "colorsMap='jet'\n",
    "import matplotlib\n",
    "import matplotlib.cm as cmx\n",
    "import glob\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "def pairwise_dist(xt,y_p):\n",
    "    a=xt.shape[1]\n",
    "    b=y_p.shape[1]\n",
    "    dist=tf.tile(tf.expand_dims(y_p,1),[1,a,1,1])-tf.tile(tf.expand_dims(xt,2),[1,1,b,1])\n",
    "    dist=(dist[:,:,:,0]**2+dist[:,:,:,1]**2)\n",
    "    return dist\n",
    "\n",
    "def chamfer_loss(A,B):    \n",
    "    r=tf.reduce_sum(A*A,2)\n",
    "    r=tf.reshape(r,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    r2=tf.reduce_sum(B*B,2)\n",
    "    r2=tf.reshape(r2,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    t=(r-2*tf.matmul(A, tf.transpose(B,perm=[0, 2, 1])) + tf.transpose(r2,perm=[0, 2, 1]))\n",
    "    return tf.reduce_mean((tf.reduce_min(t, axis=1)+tf.reduce_min(t,axis=2))/2.0)\n",
    "\n",
    "def Net(aa, yt, x):\n",
    "    s=aa.shape[1]\n",
    "    with tf.sg_context(name='NNReg', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        yt=tf.expand_dims(yt,2)\n",
    "        \n",
    "        v1=tf.expand_dims(x,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        vv5=v5\n",
    "        \n",
    "        v1=yt.sg_conv(dim=16, size=(1,1),  name='gen99',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1),  name='gen11',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1),  name='gen22',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1),  name='gen33',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1),  name='gen44',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(aa,2),v5], axis=-1) \n",
    "        ff=tf.concat([ff,vv5], axis=-1) \n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        \n",
    "        f3=f2.sg_conv(dim=3, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\")  \n",
    "        f3=tf.squeeze(f3,axis=2)\n",
    "        \n",
    "    return f3\n",
    "\n",
    "def part_set(y):#used in training, deformed shape\n",
    "    with tf.sg_context(name='RegSeg1', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        v1=tf.expand_dims(y,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,int(y.shape[1]),1,1]) #[B,N,1,1024]\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(y,2),v5], axis=-1) #[B,N,1,3+1024]\n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f1=tf.concat([tf.expand_dims(y,2),f1], axis=-1) \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        f2=tf.concat([tf.expand_dims(y,2),f2], axis=-1) \n",
    "        feat=f2.sg_conv(dim=128, size=(1,1),  name='feat',pad=\"SAME\",bn=True)\n",
    "        f3=feat.sg_conv(dim=4, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\") \n",
    "        f3=tf.squeeze(f3,axis=2) #[B,N,4]\n",
    "        feat = tf.squeeze(feat,axis=2)\n",
    "    return f3, feat\n",
    "\n",
    "def part_set_pred(yp, y): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='RegSeg1', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        \n",
    "        v1=tf.expand_dims(y,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,int(y.shape[1]),1,1])\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(yp,2),v5], axis=-1) \n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f1=tf.concat([tf.expand_dims(yp,2),f1], axis=-1) \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        f2=tf.concat([tf.expand_dims(yp,2),f2], axis=-1) \n",
    "        feat=f2.sg_conv(dim=128, size=(1,1),  name='feat',pad=\"SAME\",bn=True)\n",
    "        f3=feat.sg_conv(dim=4, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\") \n",
    "        f3=tf.squeeze(f3,axis=2)\n",
    "        feat = tf.squeeze(feat,axis=2)\n",
    "    return f3\n",
    "\n",
    "def part_encoder(feat): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='part_encoder', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        \n",
    "        v1=tf.expand_dims(feat,2).sg_conv(dim=128, size=(1,1),  name='part_en1',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=128, size=(1,1), name='part_en2',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=256, size=(1,1), name='part_en3',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=1024, size=(1,1), name='part_en4',pad=\"SAME\",bn=True) \n",
    "        v4=tf.expand_dims(tf.reduce_max(v4, axis=1),axis=1) #[B,1,1,1024]\n",
    "        v4=tf.squeeze(v4,axis=1)\n",
    "        v4=tf.squeeze(v4,axis=1) #[B,1024]\n",
    "    return v4\n",
    "\n",
    "def part_encoder_v2(feat): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='part_encoder', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        v1 = tf.expand_dims(feat,0)\n",
    "        v1 = tf.expand_dims(v1,2)\n",
    "        v1 = v1.sg_conv(dim=128, size=(1,1),  name='part_en1',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=128, size=(1,1), name='part_en2',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=256, size=(1,1), name='part_en3',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='part_en4',pad=\"SAME\",bn=True) \n",
    "        v4=tf.expand_dims(tf.reduce_max(v4, axis=1),axis=1) #[B,1,1,1024]\n",
    "        v4=tf.squeeze(v4,axis=1)\n",
    "        v4=tf.squeeze(v4) #[1,1024]\n",
    "    return v4\n",
    "\n",
    "def PearsonCoeff(x,y):\n",
    "    vx = x - tf.reduce_sum(x)\n",
    "    vy = y - tf.reduce_sum(y)\n",
    "\n",
    "    cost = tf.reduce_sum(vx * vy) / (tf.sqrt(tf.reduce_sum(vx ** 2)) * tf.sqrt(tf.reduce_sum(vy ** 2)))\n",
    "    return cost\n",
    "\n",
    "def train():\n",
    "#     tf.reset_default_graph()\n",
    "    tf.set_random_seed(888)\n",
    "    print(\"*****************************************\")\n",
    "    print(\"Training started with random seed: {}\".format(111))\n",
    "    print(\"Batch started with random seed: {}\".format(111))\n",
    "    \n",
    "    #read data\n",
    "    y,l,x,xl=Utils.read_from_tfrecords(tfname,\n",
    "                                 [\"D\",\"P\",\"T\",\"TP\"], batSize, [[s1,3],[s1],[s1,3],[s1]])\n",
    "    global_step = tf.Variable(1, trainable=False,name='global_step')\n",
    "    yp=Net(x,x,y)+x #deformed shape\n",
    "    \n",
    "    #part seg during training\n",
    "    plogit, part_feat=part_set(yp) #[B,N,4], [B,N,64]\n",
    "    pred_cls = tf.argmax(plogit, axis=-1)\n",
    "    \n",
    "    #prediction during testing\n",
    "    pl = tf.argmax(part_set_pred(y,yp), axis=-1)\n",
    "    # pl=tf.argmax(part_set(yp), axis=-1)\n",
    "    \n",
    "    cross_ent_ls=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=plogit, labels=tf.cast(xl,dtype=tf.int32)))\n",
    "    chamf_ls=chamfer_loss(yp,y)  \n",
    "    print('chamf_ls, cross_ent_ls', chamf_ls, cross_ent_ls)\n",
    "    \n",
    "    lambda_ratio_tf = tf.train.exponential_decay(lambda_ratio, global_step,\n",
    "                                                  batSize, 0.999, staircase=False) \n",
    "    Loss =  cross_ent_ls + chamf_ls*lambda_ratio_tf\n",
    "    \n",
    "    #correlation loss \n",
    "    print('part_feat.shape: ',part_feat)\n",
    "    \n",
    "    M_list = []\n",
    "    E_same_list = []\n",
    "    E_diff_list = []\n",
    "    \n",
    "    for c_id in range(4):\n",
    "        M_c = []\n",
    "        for s_id in range(batSize):\n",
    "            pred_cls_s = pred_cls[s_id] #[N,4]\n",
    "            mask0 = tf.boolean_mask(part_feat[s_id], tf.equal(pred_cls_s,c_id))\n",
    "#             def f1(): return part_encoder_v2(mask0) #[1,512]\n",
    "            def f1(): return tf.reduce_max(mask0, axis=0) #[1,128]\n",
    "            def f2(): return tf.ones([128,])\n",
    "            mask0 = tf.cond(tf.reduce_any(tf.equal(pred_cls_s,c_id)), f1, f2)\n",
    "            M_c.append(mask0)\n",
    "            \n",
    "        M_c = tf.stack(M_c) #[B,128]\n",
    "        M_list.append(tf.reduce_mean(M_c, axis=0))\n",
    "        corr_same_list = []\n",
    "        for ii in range(batSize):\n",
    "            for jj in range(ii+1, batSize):\n",
    "                corr_same_list.append(PearsonCoeff(M_c[ii], M_c[jj]))\n",
    "        E_same = tf.reduce_min(corr_same_list)\n",
    "        E_same_list.append(E_same)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(i+1,4):\n",
    "            pearson_r = PearsonCoeff(M_list[i], M_list[j])\n",
    "            E_diff_list.append(pearson_r)\n",
    "    \n",
    "    E_corr = -tf.reduce_mean(E_same_list) + tf.reduce_max(E_diff_list)\n",
    "    \n",
    "#     Loss = Loss + beta_ratio*E_corr\n",
    "    \n",
    "    def f1(): return Loss + E_corr* beta_ratio\n",
    "    def f2(): return Loss\n",
    "    \n",
    "    Loss = tf.cond(global_step>5000, f1, f2) #从5000step开始使用correlation loss\n",
    "\n",
    "    #Learning Rate****************************************************************************\n",
    "    lr = tf.train.exponential_decay(learningRate, global_step,\n",
    "                                                  batSize, learningRateDecay, staircase=False) \n",
    "    # Optimization Algo************************************************************************\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                                                    beta1=adam_beta1,\n",
    "                                                    beta2=adam_beta2\n",
    "                                                   ).minimize(Loss,global_step=global_step,var_list=[i for i in \n",
    "                                                                                                     tf.trainable_variables()])\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=int(maxKeepWeights))\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                               tf.local_variables_initializer())\n",
    "    \n",
    "    # Continue Training************************************************************************\n",
    "    if len(conWeightPath)>0:\n",
    "        print(\"Continue Training...\")\n",
    "        tmp_var_list={}\n",
    "        if len(conWeightVar)==0:\n",
    "            print(\"For all variables\")\n",
    "            globals()['conWeightVar']={''}\n",
    "        else:\n",
    "            print(\"Training variables: {}\".format(conWeightVar))\n",
    "            \n",
    "        for j in conWeightVar: \n",
    "            for i in tf.global_variables():\n",
    "                if i.name.startswith(j):\n",
    "                    tmp_var_list[i.name[:-2]] = i      \n",
    "                    \n",
    "        saver1=tf.train.Saver(tmp_var_list)     \n",
    "    \n",
    "    num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print('num_params', num_params)\n",
    "    \n",
    "    \n",
    "    # Training**********************************************************************************    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # Read Weight******************************\n",
    "        if len(conWeightPath)>0:\n",
    "            print(conWeightPath)\n",
    "            if stepsContinue==-1:            \n",
    "                STEPS=sorted([int(i.split(\"/\")[-1].split(\".\")[1].split(\"-\")[-1]) for i in glob.glob(conWeightPath+\"/*meta\")])\n",
    "                print(\"hahaha\",STEPS)\n",
    "                globals()['stepsContinue']=STEPS[-1]\n",
    "                \n",
    "            wtt=glob.glob(conWeightPath+\"/*{}*meta\".format(stepsContinue))[0][:-5]\n",
    "            print(\"Reading Weight:{}\".format(wtt))\n",
    "            saver1.restore(sess,wtt)\n",
    "            print('Weight is successfully updated from: {}'.format(wtt))  \n",
    "        #*******************************************    \n",
    "        stepst = sess.run(global_step)\n",
    "        for t in tqdm.tqdm(range(stepst,int(maxStep)+1)):      \n",
    "            _= sess.run([train_step]) \n",
    "            if t % saveStep==0:\n",
    "                if not os.path.exists(dirSave):\n",
    "                    os.makedirs(dirSave)\n",
    "                saver.save(sess, dirSave + '/model.ckpt', global_step=t)\n",
    "            \n",
    "            if t%200==0:\n",
    "                print('test', sess.run([tf.reduce_mean(E_same_list), tf.reduce_max(E_diff_list)]))\n",
    "                \n",
    "            if t%100==0:\n",
    "                chamf_ls_val, lambda_ratio_val, cross_ent_ls_val, total_ls_val=sess.run([chamf_ls, lambda_ratio_tf, cross_ent_ls, Loss])\n",
    "                print('chamf_ls: {:.5f}, cd weight: {:.5f}, cross_ent_ls: {:.5f}, correlation loss: {:.5f}, total_loss: {:.5f}'.format(chamf_ls_val, lambda_ratio_val, cross_ent_ls_val, total_ls_val-chamf_ls_val-cross_ent_ls_val, total_ls_val))\n",
    "                \n",
    "#             if t % printStep==0:\n",
    "#                 yyp,yy,ll,ppl,xx,xxl=sess.run([yp,y,l,pl,x,xl])  #deformed shape, input shape, input label, prediction label, template shape, template label\n",
    "#                 plot_result(yyp,yy,ll,ppl,xx,xxl)\n",
    "                \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "train_num=10000\n",
    "batSize=16\n",
    "maxStep=50000 # fixed with learningRate and learningRateDecay\n",
    "learningRate=0.001\n",
    "learningRateDecay=0.995\n",
    "lrType=1 #learning rate type\n",
    "lr2Period=5\n",
    "adam_beta1=0.9 # check adam optimization\n",
    "adam_beta2=0.99\n",
    "conWeightVar=['NNReg'] # variables to be loaded\n",
    "saveStep=200 # frequency to save weight\n",
    "maxKeepWeights=2000 # how many records to save (for disk)\n",
    "stepsContinue=100000  # from which steps continu.\n",
    "# conWeightPath=\"./NIPS-partseg-v1-temp10-chair/Exp4_maxStep_100000/\"\n",
    "conWeightPath=\"\"\n",
    "#For Debug and results printing\n",
    "keepProb=0.99999\n",
    "# Totally dosen't work if put dropout after max-pool layer. \n",
    "printStep=500\n",
    "\n",
    "clas=\"mug-seg\"\n",
    "s1=2048\n",
    "tfname=\"./mug_N2048.tfrecords\"\n",
    "dat=\"LX-run1\"\n",
    "lambda_ratio = 100.0 # init weight for chamfer distance \n",
    "beta_ratio = 0.1 # weight for correlation loss\n",
    "\n",
    "dirSave=\"./{}_trNum_{}_maxStep_{}_lambda_{}\".format(clas,\n",
    "    dat,train_num, maxStep,lambda_ratio)\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Training started with random seed: 111\n",
      "Batch started with random seed: 111\n",
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:148: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:149: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/donglin/Github/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation/Utils.py:165: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /home/donglin/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/sugartensor/sg_activation.py:40: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "chamf_ls, cross_ent_ls Tensor(\"Mean_1:0\", shape=(), dtype=float32) Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /tmp/ipykernel_63758/4074143391.py:168: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "part_feat.shape:  Tensor(\"RegSeg1/Squeeze_1:0\", shape=(16, 2048, 128), dtype=float32)\n",
      "WARNING:tensorflow:From /tmp/ipykernel_63758/4074143391.py:217: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_63758/4074143391.py:223: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "num_params 2681079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:39:43.085045: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-14 19:39:43.104980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 19:39:43.105068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.695\n",
      "pciBusID: 0000:2b:00.0\n",
      "2022-03-14 19:39:43.105109: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.105139: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.105166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.105191: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.105217: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.105244: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 19:39:43.106955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-03-14 19:39:43.106963: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\n",
      "2022-03-14 19:39:43.107360: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-03-14 19:39:43.130027: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3799885000 Hz\n",
      "2022-03-14 19:39:43.130425: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56244101cfc0 executing computations on platform Host. Devices:\n",
      "2022-03-14 19:39:43.130437: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2022-03-14 19:39:43.130481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-14 19:39:43.130486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      \n",
      "2022-03-14 19:39:43.194429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 19:39:43.194619: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56243db7a3a0 executing computations on platform CUDA. Devices:\n",
      "2022-03-14 19:39:43.194630: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_63758/4074143391.py:252: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:39:44.951913: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "  0%|          | 20/50000 [01:38<68:18:39,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[{{node input_producer/input_producer_EnqueueMany}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-03-14 19:41:24.863421: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\n",
      "2022-03-14 19:41:24.863469: W tensorflow/core/kernels/queue_base.cc:277] _0_input_producer: Skipping cancelled enqueue attempt with queue not closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63758/3364925475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_63758/4074143391.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mstepst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstepst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msaveStep\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirSave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp-seg37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ed72bcfecfd46c85cf5baaaf269242447cba67c4933d78848127969168b65d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('temp-seg37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
