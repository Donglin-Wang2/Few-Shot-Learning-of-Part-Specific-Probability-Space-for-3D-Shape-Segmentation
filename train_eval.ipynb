{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#Author\n",
    "#Lingjing Wang and Xiang Li\n",
    "#Date: June 2020\n",
    "############################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import Utils\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import sugartensor as tf # Please use pip install for this small package\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "colorsMap='jet'\n",
    "import matplotlib\n",
    "import matplotlib.cm as cmx\n",
    "import glob\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "def pairwise_dist(xt,y_p):\n",
    "    a=xt.shape[1]\n",
    "    b=y_p.shape[1]\n",
    "    dist=tf.tile(tf.expand_dims(y_p,1),[1,a,1,1])-tf.tile(tf.expand_dims(xt,2),[1,1,b,1])\n",
    "    dist=(dist[:,:,:,0]**2+dist[:,:,:,1]**2)\n",
    "    return dist\n",
    "\n",
    "def chamfer_loss(A,B):    \n",
    "    r=tf.reduce_sum(A*A,2)\n",
    "    r=tf.reshape(r,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    r2=tf.reduce_sum(B*B,2)\n",
    "    r2=tf.reshape(r2,[int(r.shape[0]),int(r.shape[1]),1])\n",
    "    t=(r-2*tf.matmul(A, tf.transpose(B,perm=[0, 2, 1])) + tf.transpose(r2,perm=[0, 2, 1]))\n",
    "    return tf.reduce_mean((tf.reduce_min(t, axis=1)+tf.reduce_min(t,axis=2))/2.0)\n",
    "\n",
    "def Net(aa, yt, x):\n",
    "    s=aa.shape[1]\n",
    "    with tf.sg_context(name='NNReg', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        yt=tf.expand_dims(yt,2)\n",
    "        \n",
    "        v1=tf.expand_dims(x,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        vv5=v5\n",
    "        \n",
    "        v1=yt.sg_conv(dim=16, size=(1,1),  name='gen99',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1),  name='gen11',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1),  name='gen22',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1),  name='gen33',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1),  name='gen44',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,s,1,1])\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(aa,2),v5], axis=-1) \n",
    "        ff=tf.concat([ff,vv5], axis=-1) \n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        \n",
    "        f3=f2.sg_conv(dim=3, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\")  \n",
    "        f3=tf.squeeze(f3,axis=2)\n",
    "        \n",
    "    return f3\n",
    "\n",
    "def part_set(y):#used in training, deformed shape\n",
    "    with tf.sg_context(name='RegSeg1', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        v1=tf.expand_dims(y,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,int(y.shape[1]),1,1]) #[B,N,1,1024]\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(y,2),v5], axis=-1) #[B,N,1,3+1024]\n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f1=tf.concat([tf.expand_dims(y,2),f1], axis=-1) \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        f2=tf.concat([tf.expand_dims(y,2),f2], axis=-1) \n",
    "        feat=f2.sg_conv(dim=128, size=(1,1),  name='feat',pad=\"SAME\",bn=True)\n",
    "        f3=feat.sg_conv(dim=4, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\") \n",
    "        f3=tf.squeeze(f3,axis=2) #[B,N,4]\n",
    "        feat = tf.squeeze(feat,axis=2)\n",
    "    return f3, feat\n",
    "\n",
    "def part_set_pred(yp, y): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='RegSeg1', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        \n",
    "        v1=tf.expand_dims(y,2).sg_conv(dim=16, size=(1,1),  name='gen9',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=64, size=(1,1), name='gen1',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=128, size=(1,1), name='gen2',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='gen3',pad=\"SAME\",bn=True) \n",
    "        v5=v4.sg_conv(dim=1024, size=(1,1), name='gen4',pad=\"SAME\",bn=True) \n",
    "        v5=tf.tile(tf.expand_dims(tf.reduce_max(v5, axis=1),axis=1),[1,int(y.shape[1]),1,1])\n",
    "        \n",
    "        ff=tf.concat([tf.expand_dims(yp,2),v5], axis=-1) \n",
    "        f1=ff.sg_conv(dim=256, size=(1,1),  name='f1',pad=\"SAME\",bn=True)  \n",
    "        f1=tf.concat([tf.expand_dims(yp,2),f1], axis=-1) \n",
    "        f2=f1.sg_conv(dim=128, size=(1,1),  name='f2',pad=\"SAME\",bn=True)  \n",
    "        f2=tf.concat([tf.expand_dims(yp,2),f2], axis=-1) \n",
    "        feat=f2.sg_conv(dim=128, size=(1,1),  name='feat',pad=\"SAME\",bn=True)\n",
    "        f3=feat.sg_conv(dim=4, size=(1,1),  name='f3',pad=\"SAME\",bn=False, act=\"linear\") \n",
    "        f3=tf.squeeze(f3,axis=2)\n",
    "        feat = tf.squeeze(feat,axis=2)\n",
    "    return f3\n",
    "\n",
    "def part_encoder(feat): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='part_encoder', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        \n",
    "        v1=tf.expand_dims(feat,2).sg_conv(dim=128, size=(1,1),  name='part_en1',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=128, size=(1,1), name='part_en2',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=256, size=(1,1), name='part_en3',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=1024, size=(1,1), name='part_en4',pad=\"SAME\",bn=True) \n",
    "        v4=tf.expand_dims(tf.reduce_max(v4, axis=1),axis=1) #[B,1,1,1024]\n",
    "        v4=tf.squeeze(v4,axis=1)\n",
    "        v4=tf.squeeze(v4,axis=1) #[B,1024]\n",
    "    return v4\n",
    "\n",
    "def part_encoder_v2(feat): #used in testing, input shape, deformed shape\n",
    "    with tf.sg_context(name='part_encoder', stride=1, act='leaky_relu', bn=True, reuse=tf.AUTO_REUSE): \n",
    "        v1 = tf.expand_dims(feat,0)\n",
    "        v1 = tf.expand_dims(v1,2)\n",
    "        v1 = v1.sg_conv(dim=128, size=(1,1),  name='part_en1',pad=\"SAME\",bn=True)        \n",
    "        v2=v1.sg_conv(dim=128, size=(1,1), name='part_en2',pad=\"SAME\",bn=True)        \n",
    "        v3=v2.sg_conv(dim=256, size=(1,1), name='part_en3',pad=\"SAME\",bn=True) \n",
    "        v4=v3.sg_conv(dim=512, size=(1,1), name='part_en4',pad=\"SAME\",bn=True) \n",
    "        v4=tf.expand_dims(tf.reduce_max(v4, axis=1),axis=1) #[B,1,1,1024]\n",
    "        v4=tf.squeeze(v4,axis=1)\n",
    "        v4=tf.squeeze(v4) #[1,1024]\n",
    "    return v4\n",
    "\n",
    "def PearsonCoeff(x,y):\n",
    "    vx = x - tf.reduce_sum(x)\n",
    "    vy = y - tf.reduce_sum(y)\n",
    "\n",
    "    cost = tf.reduce_sum(vx * vy) / (tf.sqrt(tf.reduce_sum(vx ** 2)) * tf.sqrt(tf.reduce_sum(vy ** 2)))\n",
    "    return cost\n",
    "\n",
    "def train():\n",
    "#     tf.reset_default_graph()\n",
    "    tf.set_random_seed(888)\n",
    "    print(\"*****************************************\")\n",
    "    print(\"Training started with random seed: {}\".format(111))\n",
    "    print(\"Batch started with random seed: {}\".format(111))\n",
    "    \n",
    "    #read data\n",
    "    y,l,x,xl=Utils.read_from_tfrecords(tfname,\n",
    "                                 [\"D\",\"P\",\"T\",\"TP\"], batSize, [[s1,3],[s1],[s1,3],[s1]])\n",
    "    global_step = tf.Variable(1, trainable=False,name='global_step')\n",
    "    yp=Net(x,x,y)+x #deformed shape\n",
    "    \n",
    "    #part seg during training\n",
    "    plogit, part_feat=part_set(yp) #[B,N,4], [B,N,64]\n",
    "    pred_cls = tf.argmax(plogit, axis=-1)\n",
    "    \n",
    "    #prediction during testing\n",
    "    pl = tf.argmax(part_set_pred(y,yp), axis=-1)\n",
    "    # pl=tf.argmax(part_set(yp), axis=-1)\n",
    "    \n",
    "    cross_ent_ls=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=plogit, labels=tf.cast(xl,dtype=tf.int32)))\n",
    "    chamf_ls=chamfer_loss(yp,y)  \n",
    "    print('chamf_ls, cross_ent_ls', chamf_ls, cross_ent_ls)\n",
    "    \n",
    "    lambda_ratio_tf = tf.train.exponential_decay(lambda_ratio, global_step,\n",
    "                                                  batSize, 0.999, staircase=False) \n",
    "    Loss =  cross_ent_ls + chamf_ls*lambda_ratio_tf\n",
    "    \n",
    "    #correlation loss \n",
    "    print('part_feat.shape: ',part_feat)\n",
    "    \n",
    "    M_list = []\n",
    "    E_same_list = []\n",
    "    E_diff_list = []\n",
    "    \n",
    "    for c_id in range(4):\n",
    "        M_c = []\n",
    "        for s_id in range(batSize):\n",
    "            pred_cls_s = pred_cls[s_id] #[N,4]\n",
    "            mask0 = tf.boolean_mask(part_feat[s_id], tf.equal(pred_cls_s,c_id))\n",
    "#             def f1(): return part_encoder_v2(mask0) #[1,512]\n",
    "            def f1(): return tf.reduce_max(mask0, axis=0) #[1,128]\n",
    "            def f2(): return tf.ones([128,])\n",
    "            mask0 = tf.cond(tf.reduce_any(tf.equal(pred_cls_s,c_id)), f1, f2)\n",
    "            M_c.append(mask0)\n",
    "            \n",
    "        M_c = tf.stack(M_c) #[B,128]\n",
    "        M_list.append(tf.reduce_mean(M_c, axis=0))\n",
    "        corr_same_list = []\n",
    "        for ii in range(batSize):\n",
    "            for jj in range(ii+1, batSize):\n",
    "                corr_same_list.append(PearsonCoeff(M_c[ii], M_c[jj]))\n",
    "        E_same = tf.reduce_min(corr_same_list)\n",
    "        E_same_list.append(E_same)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(i+1,4):\n",
    "            pearson_r = PearsonCoeff(M_list[i], M_list[j])\n",
    "            E_diff_list.append(pearson_r)\n",
    "    \n",
    "    E_corr = -tf.reduce_mean(E_same_list) + tf.reduce_max(E_diff_list)\n",
    "    \n",
    "#     Loss = Loss + beta_ratio*E_corr\n",
    "    \n",
    "    def f1(): return Loss + E_corr* beta_ratio\n",
    "    def f2(): return Loss\n",
    "    \n",
    "    Loss = tf.cond(global_step>5000, f1, f2) #从5000step开始使用correlation loss\n",
    "\n",
    "    #Learning Rate****************************************************************************\n",
    "    lr = tf.train.exponential_decay(learningRate, global_step,\n",
    "                                                  batSize, learningRateDecay, staircase=False) \n",
    "    # Optimization Algo************************************************************************\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                                                    beta1=adam_beta1,\n",
    "                                                    beta2=adam_beta2\n",
    "                                                   ).minimize(Loss,global_step=global_step,var_list=[i for i in \n",
    "                                                                                                     tf.trainable_variables()])\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=int(maxKeepWeights))\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                               tf.local_variables_initializer())\n",
    "    \n",
    "    # Continue Training************************************************************************\n",
    "    if len(conWeightPath)>0:\n",
    "        print(\"Continue Training...\")\n",
    "        tmp_var_list={}\n",
    "        if len(conWeightVar)==0:\n",
    "            print(\"For all variables\")\n",
    "            globals()['conWeightVar']={''}\n",
    "        else:\n",
    "            print(\"Training variables: {}\".format(conWeightVar))\n",
    "            \n",
    "        for j in conWeightVar: \n",
    "            for i in tf.global_variables():\n",
    "                if i.name.startswith(j):\n",
    "                    tmp_var_list[i.name[:-2]] = i      \n",
    "                    \n",
    "        saver1=tf.train.Saver(tmp_var_list)     \n",
    "    \n",
    "    num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print('num_params', num_params)\n",
    "    \n",
    "    \n",
    "    # Training**********************************************************************************    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # Read Weight******************************\n",
    "        if len(conWeightPath)>0:\n",
    "            print(conWeightPath)\n",
    "            if stepsContinue==-1:            \n",
    "                STEPS=sorted([int(i.split(\"/\")[-1].split(\".\")[1].split(\"-\")[-1]) for i in glob.glob(conWeightPath+\"/*meta\")])\n",
    "                print(\"hahaha\",STEPS)\n",
    "                globals()['stepsContinue']=STEPS[-1]\n",
    "                \n",
    "            wtt=glob.glob(conWeightPath+\"/*{}*meta\".format(stepsContinue))[0][:-5]\n",
    "            print(\"Reading Weight:{}\".format(wtt))\n",
    "            saver1.restore(sess,wtt)\n",
    "            print('Weight is successfully updated from: {}'.format(wtt))  \n",
    "        #*******************************************    \n",
    "        stepst = sess.run(global_step)\n",
    "        for t in tqdm.tqdm(range(stepst,int(maxStep)+1)):      \n",
    "            _= sess.run([train_step]) \n",
    "            if t % saveStep==0:\n",
    "                if not os.path.exists(dirSave):\n",
    "                    os.makedirs(dirSave)\n",
    "                saver.save(sess, dirSave + '/model.ckpt', global_step=t)\n",
    "            \n",
    "            if t%200==0:\n",
    "                print('test', sess.run([tf.reduce_mean(E_same_list), tf.reduce_max(E_diff_list)]))\n",
    "                \n",
    "            if t%100==0:\n",
    "                chamf_ls_val, lambda_ratio_val, cross_ent_ls_val, total_ls_val=sess.run([chamf_ls, lambda_ratio_tf, cross_ent_ls, Loss])\n",
    "                print('chamf_ls: {:.5f}, cd weight: {:.5f}, cross_ent_ls: {:.5f}, correlation loss: {:.5f}, total_loss: {:.5f}'.format(chamf_ls_val, lambda_ratio_val, cross_ent_ls_val, total_ls_val-chamf_ls_val-cross_ent_ls_val, total_ls_val))\n",
    "                \n",
    "#             if t % printStep==0:\n",
    "#                 yyp,yy,ll,ppl,xx,xxl=sess.run([yp,y,l,pl,x,xl])  #deformed shape, input shape, input label, prediction label, template shape, template label\n",
    "#                 plot_result(yyp,yy,ll,ppl,xx,xxl)\n",
    "                \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "train_num=10000\n",
    "batSize=16\n",
    "maxStep=50000 # fixed with learningRate and learningRateDecay\n",
    "learningRate=0.001\n",
    "learningRateDecay=0.995\n",
    "lrType=1 #learning rate type\n",
    "lr2Period=5\n",
    "adam_beta1=0.9 # check adam optimization\n",
    "adam_beta2=0.99\n",
    "conWeightVar=['NNReg'] # variables to be loaded\n",
    "saveStep=200 # frequency to save weight\n",
    "maxKeepWeights=2000 # how many records to save (for disk)\n",
    "stepsContinue=100000  # from which steps continu.\n",
    "# conWeightPath=\"./NIPS-partseg-v1-temp10-chair/Exp4_maxStep_100000/\"\n",
    "conWeightPath=\"\"\n",
    "#For Debug and results printing\n",
    "keepProb=0.99999\n",
    "# Totally dosen't work if put dropout after max-pool layer. \n",
    "printStep=500\n",
    "\n",
    "clas=\"chair-seg\"\n",
    "s1=2048\n",
    "tfname=\"./chair_N2048.tfrecords\"\n",
    "dat=\"LX-run1\"\n",
    "lambda_ratio = 100.0 # init weight for chamfer distance \n",
    "beta_ratio = 0.1 # weight for correlation loss\n",
    "\n",
    "dirSave=\"./{}_trNum_{}_maxStep_{}_lambda_{}\".format(clas,\n",
    "    dat,train_num, maxStep,lambda_ratio)\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ed72bcfecfd46c85cf5baaaf269242447cba67c4933d78848127969168b65d0"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
